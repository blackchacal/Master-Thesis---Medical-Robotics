%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with the literature review
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Literature Review}
\label{cha:literature_review}

\begin{quotation}
\begin{flushright}
\itshape
«The man who does not read has no advantage over the man who cannot read.»\\
\textbf{- Mark Twain}
\end{flushright}
\end{quotation}

3D Bioprinting is a promising technology to solve the increasing demand for organ donation, full-thickness skin replacements and in vitro tissue models. However, it still faces a few limitations throughout its three stages of the bioprinting process. This work intends to propose a solution to some of the problems associated with the bioprinting stage, in particular, the way the bioprinter works.

This chapter will focus on presenting the state-of-the-art on the positioning systems used for 3D bioprinting (both external and internal), and how that relates to resolution, speed and in situ bioprinting. It will also focus on the state-of-the-art of using vision and sensing systems to increase the system's autonomy by detecting the printing site or evaluating the printing process in real-time. It will also consider the existing wound segmentation algorithms that can be used for system automation.

% ==========================
% = 3D Positioning Systems =
% ==========================

\section{3D Positioning systems}
\label{sec:3d_positioning_systems}

The positioning system of the bioprinter is one of the most important factors on printing resolution, speed and ability to bioprint in situ. Because of this, the improvement of the bioprinter abilities depends directly on the capacity of this system.

Three dimensional bioprinters come in different shapes and sizes. There exists commercial bioprinters, bioprinters developed solely for research purposes, and open source bioprinters. Although all different, there is something that unite most of the existing systems. They use a gantry or Cartesian system as the positioning system. This only allows printing to occur horizontally and mostly on planar surfaces. Despite their limitations to work on non-planar surfaces, there were some success cases reported \cite{Lee2009_multi_layer_culture_human_skin_non_planar}.

In 2016, \citeauthor{Ozbolat2017_evaluation_bioprinter_tech} \cite{Ozbolat2017_evaluation_bioprinter_tech}, made a comprehensive review of existing bioprinter technologies. The authors mentioned ten capabilities of an ideal bioprinter. These are high \gls{dof} in motion; high resolution and accuracy; high speed motion; the ability to dispense various bioink solutions simultaneously; ease of use; compact size; ease of sterilization; full-automation capability; affordability; and versatility. None of the bioprinters assessed had all these capabilities.

This thesis will focus on the first three capabilities and also full-automation. High \gls{dof} in motion allows bioprinting on non-planar surfaces. It is crucial for the transition into clinical applications where cells are bioprinted into a biological lesion site \cite{Ozbolat2017_evaluation_bioprinter_tech}. High resolution and accuracy are important to "enable deposition of bioink solutions with sufficient fidelity to simulate cell placement in native tissues" \cite{Ozbolat2017_evaluation_bioprinter_tech}. If one of the goals is to have "rapid fabrication of human-scale tissue and organ constructs for clinical transplantation as well as high-throughput production of tissue models for pharmaceutics and cancer research" \cite{Ozbolat2017_evaluation_bioprinter_tech}, high speed bioprinting is vital and full-automation can help. In several steps, removing the human out of the loop can increase efficiency.

On the article just mentioned, altogether, fifty-eight bioprinters were evaluated, 17 \gls{ebb}, 33 \gls{dbb} and 8 \gls{lbb} systems. \gls{ebb} systems were all commercial. \gls{dbb} systems were 7 commercial and the rest non-commercial systems. \gls{lbb} systems were all non-commercial. No commercial \gls{lbb} system was available on the market at that time.

Of all bioprinters analysed only one had more than 3 \gls{dof}. This bioprinter was BioAssemblyBot\textregistered{} which is a commercial system developed by US-based company Advanced Solutions, Inc. It uses a custom-made 6 \gls{dof} robotic arm as its 3D positioning system \cite{Advanced2020_bioassemblybot}. Another interesting aspect is that none of the bioprinters assessed were used for in situ bioprinting. All of them bioprint the construct inside their building volume, which is later transferred to its final destination.

BioAssemblyBot\textregistered{} is a versatile bioprinting system. It is able to print cell systems and 3D assays; experimental tissue models and microenvironments; organ models; microfluidic platforms; implant systems \cite{Advanced2020_bioassemblybot}. It has a build envelop of 250 mm (w) $\times$ 300 mm (d) $\times$ 250 mm (h) and uses up to 8 different tools. One of the main advantages is that it is able to do other tasks aside of additive 3D bioprinting. It can also do contour 3D printing, pick-and-place and assembling. The main limitation of this system is that it cannot be used to bioprint in situ on humans. Although the robotic arm would allow it to deal with the challenges of non-planar surfaces, it was conceived to work inside its own clean room environment \cite{Advanced2020_bioassemblybot}. It could be used for in situ bioprinting only for small animals that could fit into the working volume. There is no information available regarding system and printing resolution, or speed.

Having positioning systems with more than 3 \gls{dof} has several advantages to the printing process. The most obvious advantage is the ability to print onto non-planar surfaces. Another advantage is dealing with patient positioning. If a system has to bioprint skin tissue in situ to an extensive burn wound, the system may need to adapt to the patient and not the opposite. This means it must adjust to the patient position and relative position of the bioprinting site. If the bioprinting site has an inclined orientation, a 3 \gls{dof} system will not be able to change its orientation and correctly do the printing work. One last advantage to mention, is considering the possibility of internal bioprinting, i.e., the capability to bioprint directly onto an internal structure like an organ or joint during a surgical procedure. The body's internal environment is highly populated with different structures. This means many obstacles to position correctly the bioprinter deposition system. Having higher degrees of freedom helps the system to evade obstacles more efficiently.\bigskip

Some research efforts tried to use the capabilities of robotic manipulators to enhance the bioprinting process, both in vitro and in situ (internally and externally).

The first work found using a robotic arm for the positioning system was from 2017, in which, \citeauthor{Li2017a_additive_manufacturing_in_situ_printing_dobot}\cite{Li2017a_additive_manufacturing_in_situ_printing_dobot}, developed a system for additive manufacturing for in situ printing. The robotic arm, Dobot version 1.0, from the Chinese company Shenzhen Yuejiang Technology Co., only has 3 \gls{dof} but a different configuration from a gantry. All the joints are revolute instead of prismatic like the gantry. The arm as a position accuracy of $\pm$ 0.1 mm. It has a custom made end-effector with a micro-valve inkjet dispenser and two UV light sources for curing the print. An experiment of in situ bioprinting was done on a 3D printed PLA rat model with a defect site on his back. The defect was a rectangular hole 15 mm (w) $\times$ 10  mm (d) $\times$ 1 mm (h). The bioink was printed at a 3 mm/s with a line spacing of 0.75 mm, using two path patterns. The printed results were good for both path patterns and were able to "fill the defect site completely" \cite{Li2017a_additive_manufacturing_in_situ_printing_dobot}. One downside was the fact that the relative positioning between the nozzle and the defect site was manually setup.

In 2018, a conference paper by \citeauthor{Jafari2018_robot_system_automated_wound_filling}\cite{Jafari2018_robot_system_automated_wound_filling}, presented the use of a robotic manipulator to do automated wound filling. The system consisted in TX90 6 \gls{dof} industrial robotic manipulator, from the Swiss company St\"{a}ubli, paired with an infrared (IR) stereo camera. The printing speed used was 1 cm/s to avoid splatter. It is relatively slow, but it was a constraint of the printing modality and not of the positioning system. There was no information about the printing resolution. An experiment done on paper, where two different inks were printed on top of each other, was used as validation of the accuracy. They presented it as evidence of better results than manual applications, but no precise measurements were shown. They also presented an experiment were they used a foam head with a wound hole that was filled with bioink. It shows that the system could potentially be used for in situ bioprinting. The novelty of this work is the use of an IR stereo camera for wound detection and path planning. This gives the system a degree of autonomy for in situ bioprinting, currently non-existent. This work seems to be the closest in goal and realisation to what is proposed on this thesis.

Another work was presented by \citeauthor{Gome2019_openlh}\cite{Gome2019_openlh} on a conference of \gls{hci} in 2019. Their work consisted in the creation of an open source liquid handling system for biological experimentation. It uses a 4 \gls{dof}, from the Chinese company UFACTORY, as the positioning system. The system is not a 3D bioprinter per se but can also be used for that to a certain level. The robot arm has a repeatability of 0.2 mm, max speed of 100 mm/s, and max resolution of 0.2 mm. Since the system was not exactly a bioprinter, the tests presented were not conceived to validate bioprinting capabilities in vitro or in situ. However, they mention the possibility of "adding UV crosslinking for printing bio-compatible hydrogels" \cite{Gome2019_openlh}, which means the system has the potential for bioprinting.

None of the groups referred did any type of extensive analysis regarding the robot modelling and path planning. \bigskip

Other groups also used more than 3 \gls{dof} for 3D bioprinting in situ but inside the body.

A group from UK, \citeauthor{Lipskas2019_robotic_assisted_3dbioprint_repairing_bone_cartilage}\cite{Lipskas2019_robotic_assisted_3dbioprint_repairing_bone_cartilage}, in 2019, developed a robotic-assisted 3D bioprinting system for bone and cartilage repairing using a minimally invasive approach. They built a custom robotic manipulator specially designed to be used on arthroscopy, a type of \gls{mis}. The manipulator has 6 \gls{dof}, with some range limits on the end-effector orientation. The system has three different end-effector, one for bone machining, another for surface registration and another for bioprinting.

Another example, also from 2019, focused on \gls{mis}, is the Endo AM system which is an endoscopic additive manufacturing tool \cite{Simeunovic2019_endoscopic_additive_manufacturing}. Conceived by \citeauthor{Simeunovic2019_endoscopic_additive_manufacturing}, the system consists on a 9 \gls{dof} robotic manipulator and an extruder. It is based on the Da Vinci Xi system. This article presents a thorough analysis of the robotic system kinematics and dynamics.\bigskip

On another tone, a review of in situ bioprinting studies from \citeauthor{Singh2020a_in_situ_bioprinting}\cite{Singh2020a_in_situ_bioprinting} presents a few approaches that handle the difficulties posed by the body non-planar shape, without multi-axis robots. The review focuses on robotic arm solutions and handheld solutions. Two examples for skin bioprinting are worth mentioning.

The first example is a handheld system developed by \citeauthor{Hakimi2018_handheld_skin_printer}\cite{Hakimi2018_handheld_skin_printer}. The system allows real-time bioprinting of skin-compatible biomaterials sheets on to the skin. Being handheld it can be controlled by the user which will take care of its positioning. Because it prints sheets the positioning does not require high precision and it can done by a person. However, it becomes a highly manual task, more suited for non-extensive wounds. In February 2020, the same group published a new paper with an improved version of the handheld bioprinter \cite{Cheng2020_handheld_skin_bioprinter}. This new version is better prepared to be used in a clinical setting.

The other example, developed by \citeauthor{Albanna2019_in_situ_bioprinting_mobile_gantry}\cite{Albanna2019_in_situ_bioprinting_mobile_gantry}, is a robotic system that uses a gantry system fixed on a mobile platform to bioprint skin constructs. The system is uncapable of dealing with inclined surfaces but the mobile plaform allows it to reach most parts of the patient's body and print in situ. A positive aspect of this system is that it can deal with extensive excisional full-thickness wounds.\bigskip

Using multi-axis robotic manipulators for 3D printing is a new trend in additive manufacturing \cite{Urhal2019_robot_assisted_additive_manufacturing_review}. However, from the literature review, the same does not seem to be the case yet on the bioprinting community. Only four systems were found using this kind of manipulators for external bioprinting. One commercial system, an open source system and two research proof-of-concept systems. Of these four, only two were used for in situ bioprinting. The other works mentioned emphasise internal body bioprinting or alternative methods for in situ bioprinting.

% section 3d_positioning_systems

\section{Vision/sensing systems and autonomy}
\label{sec:vision_sensing_systems_and_autonomy}

Vision systems are important on the 3D bioprinting scene to confer a higher degree of autonomy to the process. It can be useful both for printing site detection and evaluation, as for printing process assessment. These abilities will increase the autonomy, stated as important for the bioprinters by \citeauthor{Ozbolat2017_evaluation_bioprinter_tech}\cite{Ozbolat2017_evaluation_bioprinter_tech}, and also have the potential to increase efficacy and efficiency.\bigskip

From the works discussed on the previous section, only three used vision or sensing systems.

The first one, the BioAssemblyBot\textregistered, uses a camera inside the working space that is pointed to the working area. There is no information on the function and working of the camera available on the website. However, from watching the videos of the robot working it is possible to infer that the camera is used to monitor the printing job. It is unclear if it is for any degree of autonomy. It does not seem to be used for visual servoing because the camera does not change direction and the working envelope of the robot goes beyond the camera's field of view.

The second work that uses a vision system is from \citeauthor{Jafari2018_robot_system_automated_wound_filling}\cite{Jafari2018_robot_system_automated_wound_filling}. They specifically use an IR camera for the purpose of system automation. The camera is used to detect a wound site and plan a printing path fitted for the wound. The wounds were painted with an ink which showed up "dark under near IR frequencies" \cite{Jafari2018_robot_system_automated_wound_filling}. They also used geometrical properties to properly validate the wound site. After wound validation a point cloud was generated and fed to the path planning system. The system was not fully autonomous, because the user had to validate the detected wound site. But after that, the path planning and printing were completely autonomous.

The last work discussed was developed by \citeauthor{Albanna2019_in_situ_bioprinting_mobile_gantry}\cite{Albanna2019_in_situ_bioprinting_mobile_gantry}. In this work, a 3D scanner ZScanner{\texttrademark} Z700 scanner (3DSystems, Rock Hill, SC), was used to scan the wound geometry. The data generated by the scanner is used for wound volume calculation and printing path generation. The path planning was more precise by using the scanner data. They were even able to print two layers, one dermal layer and another epidermal layer on top.

Besides these works, two other were found where a vision system \cite{French2018_free_moving_human_anatomy_via_temporal_coarse_fine_control} and a combination of vision and laser \cite{ONeill2017_3d_bioprinting_directly_onto_moving_human_anatomy} systems were used to track the body motion during printing. The first work uses a Leap Motion sensor that tracks the user hand movement. The second work uses the laser and camera to track the movement along z-axis and a Leap Motion sensor to track X-Y axis. These works are important because they contribute directly to the autonomy of the printing system to print in situ, by dealing with the printing site movement. A simple example for the need of this type of control is to deal with the breathing movement when printing on a human torso.\bigskip

Not many bioprinting systems exploit the potential for automation given by vision and sensing systems. These systems open the door for real autonomous in situ bioprinting.

% section vision_systems_and_autonomy

\section{Burn wound segmentation}
\label{sec:burn_wound_segmentation}

In order to provide autonomy, a vision system maybe used like an RGB or depth camera. For the camera to be useful it must allow the system to detect burn wounds. Therefore, burn wound segmentation is also an important capability. Several works have been done in this field. The goal of this literature review is to show that there exists several valid approaches for burn wound segmentation that could be used. It will not go into the details of the methods. The scientific contributions analysed span the year range from 2017 to 2019. Some of the articles were not exclusively focused on burn wound segmentation but were focused on chronic wounds in general.\\

In 2017, a team from India, sensible to health problems associated with chronic wounds, proposed an algorithm for "ulcer boundary demarcation and estimation, using optical images captured by a hand-held digital camera" \cite{ManoharDhane2017_fuzzy_spectral_clustering_chronic_wounds}. The algorithm applied some pre-processing operations to the \gls{rgb} images first, and afterwards, applied fuzzy spectral clustering to the result of pre-processing for segmentation. The algorithm performance was evaluated using five similarity metrics:
Accuracy, Sensitivity, Specificity, Jaccard index and Dice coefficient. For this evaluation, ground-truth images were labeled by two experienced dermatologists and a surgeon. After testing on 70 images, the "FSC effectively segmented targeted ulcer boundary yielding 91.5\% segmentation accuracy, 86.7\%, Dice index and 79.0\%. Jaccard score. The sensitivity and specificity was found to be 87.3\% and 95.7\% respectively" \cite{ManoharDhane2017_fuzzy_spectral_clustering_chronic_wounds}. Some of the images used contained burn wounds and the segmentation results shown through images seem very good.

Also, on the same year, another team composed of scientists from China and Japan, led by \citeauthor{Lu2017_wound_intensity_correction_segmentation_cnn}, used \gls{cnn} for wound segmentation. Their approach consisted in applying some illumination and colour correction before feeding the \gls{cnn}. They compared their algorithm to other authors and their result was superior in accuracy, although from the comparison images presented, all were able to segment the wound. The problem with the other approaches was the higher detection of false positives.\\

On the next year, 2018, two other papers were published that deserved attention. The first one \cite{Li2018_composite_model_wound_segmentation_traditional_deep_nn} uses a unified approach to segmentation using traditional methods and deep neural networks. The traditional methods are used to prepare the training data. They use a skin and wound model to remove the background information from the images. These transformed images and the original ones are combined, with some extra processing, to form the training data. The data is fed to the neural network for first level segmentation. The algorithm ends with final semantic segmentation. The image set used had many different types of wounds including burn wounds. To analyse their model and compare with other authors, they choose the precision and intersection over union (IoU) metrics. Their results were better compared with the other authors.

The other paper \cite{Gholami2018_segmentation_measurements_chronic_wounds_bioprinting}, was very much related to the current thesis. Their goal was to do chronic wound segmentation and measurement for bioprinting. Instead of developing a single algorithm, they tested seven different methods from three different categories: region-based methods; edge-based methods; and texture-based methods. For evaluation eight different metrics were used: precision; accuracy; sensitivity; specificity; Jaccard index; Dice similarity; Hausdorff distance; and variance. Livewire, which is a edge-based method had the best overall score. The other important aspect of this paper, was that they were able to bioprint with "95.56\% similarity between the bioprinted patch dimensions and the desired wound geometries" \cite{Gholami2018_segmentation_measurements_chronic_wounds_bioprinting}. This paper supports the concept that it is possible to extract relevant wound geometry information from an image that allows accurate wound model bioprinting.\\

From 2019, two other are presented that focused mainly on burn wounds. The first one led by \citeauthor{Cirillo2019a_tensor_decomposition_segmentation_burn_wounds}, uses tensor decomposition for colour images. The method is based on "which effective texture features can be extracted for classification" \cite{Cirillo2019a_tensor_decomposition_segmentation_burn_wounds}. It is a direct method, which means it does not required training data. The method was compared with four different methods: CIELab; PCA; ICA; and JSEG technique by Deng and Manjunath. They use the positive predicted value (PPV) and sensitivity
(SEN) as metrics. According to the authors, "the proposed method has been shown to be able to extract burn wounds from the complex background with relatively fast computational time. The tensor decomposition is independent from the camera resolution ... [and] results in a big data reduction without any information lost for the image source estimation, and therefore applicable for real-time processing" \cite{Cirillo2019a_tensor_decomposition_segmentation_burn_wounds}.

\citeauthor{Sevik2019_automatic_classification_skin_burn_texture_base_feature_extraction} published their paper also in 2019, where a combination of clustering methods and deep learning methods were used to segment and classify, respectively, burn wound images. Four methods were tested for segmentation and 10 for classification. All the methods were able to complete their tasks. Their results show that "the best combination to successfully classify the images into skin, burn, and background regions was found to be the fuzzy c-means algorithm for the segmentation part, and a multi-layer feed-forward artificial neural network trained by the back-propagation algorithm for the classification part" \cite{Sevik2019_automatic_classification_skin_burn_texture_base_feature_extraction}.\\

These works show that there are several valid approaches to burn wound segmentation that have the potential to be used for in situ bioprinting.

% section burn_wound_segmentation